{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgVm3c7y3WXh",
        "outputId": "fcb85aed-0bba-4bdb-b699-c1caca19b304"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pip transformers\n",
        "# !pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline"
      ],
      "metadata": {
        "id": "9jfHxHrJ3gZw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = 'facebook/nllb-200-distilled-600M'"
      ],
      "metadata": {
        "id": "6od_-JFH3q9g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8N8uuR5Z37YG",
        "outputId": "a81369f8-c86b-469f-883d-4ea465494602"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translator = pipeline(\"translation\", model=model, tokenizer=tokenizer, src_lang=\"eng_Latn\", tgt_lang='tha_Thai')\n",
        "\n",
        "text_to_translate = \"Work hard, play harder\"\n",
        "\n",
        "translated_text = translator(text_to_translate)\n",
        "\n",
        "print(translated_text[0]['translation_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IdP69gm3-DV",
        "outputId": "966815c5-b8fb-47d6-e016-87cb2911067d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ทํางานหนักๆ เล่นหนักๆ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer(text_to_translate, return_tensors=\"tf\") # return_tensors=\"tf\" จะให้ผลลัพธ์เป็น TensorFlow Tensor\n",
        "\n",
        "print(tokens['input_ids'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzqhG1l34uDs",
        "outputId": "db269e02-84c9-49d7-c40c-f2d9f85abbb4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([256047  46157  19450 248079  18379 215428      2], shape=(7,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_tokens = tokenizer.decode(tokens['input_ids'][0], skip_special_tokens=False)\n",
        "\n",
        "decoded_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "09mabyRw4zVH",
        "outputId": "cc36c936-bb04-4f65-d137-51e2cc11c91c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eng_Latn Work hard, play harder</s>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_list = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0].numpy())\n",
        "\n",
        "print(text_to_translate)\n",
        "print(tokens['input_ids'][0].numpy())\n",
        "print(token_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz7PwSb34566",
        "outputId": "f954a0a2-4de9-465c-96ec-ba6d50864c07"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Work hard, play harder\n",
            "[256047  46157  19450 248079  18379 215428      2]\n",
            "['eng_Latn', '▁Work', '▁hard', ',', '▁play', '▁harder', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ข้อความภาษาไทยที่ได้จากการแปล\n",
        "thai_translated_text = translated_text[0]['translation_text']\n",
        "\n",
        "# ใช้ tokenizer เพื่อแปลงข้อความภาษาไทยเป็น token\n",
        "thai_tokens = tokenizer(thai_translated_text, return_tensors=\"tf\")\n",
        "\n",
        "# แปลง token IDs เป็น list ของ token string\n",
        "thai_token_list = tokenizer.convert_ids_to_tokens(thai_tokens['input_ids'][0].numpy())\n",
        "\n",
        "print(thai_translated_text)\n",
        "print(thai_tokens['input_ids'][0].numpy())\n",
        "print(thai_token_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAOgfWnv5ByL",
        "outputId": "8b6018d7-76b3-42ed-a0ef-e4de89fd9661"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ทํางานหนักๆ เล่นหนักๆ\n",
            "[256047  28276  24984 211830 250375  83383   7991 211830 250375      2]\n",
            "['eng_Latn', '▁ทํา', 'งาน', 'หนัก', 'ๆ', '▁เล', '่น', 'หนัก', 'ๆ', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOC5LAE45w3Y",
        "outputId": "fd7f23fb-b2e7-4384-d322-d37dfe3dad55"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "M2M100Config {\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"architectures\": [\n",
            "    \"M2M100ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"dtype\": \"float32\",\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"max_length\": 200,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"m2m_100\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"tokenizer_class\": \"NllbTokenizer\",\n",
            "  \"transformers_version\": \"4.56.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 256206\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LCzqhzkw6vB5"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}